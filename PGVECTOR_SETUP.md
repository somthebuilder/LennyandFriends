# pgvector Setup Guide

## ‚úÖ Status: pgvector is Enabled and Ready!

**pgvector works with ANY embeddings**, not just OpenAI. Your setup uses:
- **Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensions**: 384
- **Compatible**: ‚úÖ Yes, works perfectly with pgvector

## What's Already Set Up

1. ‚úÖ **pgvector extension** - Enabled in Supabase
2. ‚úÖ **chunk_embeddings table** - Has `embedding vector(384)` column
3. ‚úÖ **HNSW index** - Fast vector similarity search index
4. ‚úÖ **match_chunks function** - RPC function for vector search
5. ‚úÖ **Python code** - Already uses Supabase vector search

## How It Works

### Your Current Setup

1. **Embeddings Generation**: 
   - Uses `sentence-transformers` to generate 384-dim vectors
   - Model: `all-MiniLM-L6-v2` (free, local, fast)

2. **Storage**:
   - Embeddings stored in `chunk_embeddings.embedding` (vector type)
   - Works with any embedding model (OpenAI, sentence-transformers, etc.)

3. **Search**:
   - Uses `match_chunks()` RPC function
   - Performs cosine similarity search
   - Returns most similar chunks

### Vector Search Function

The `match_chunks` function:
- Takes a query embedding (384 dimensions)
- Searches for similar chunks using cosine distance
- Supports filtering by `guest_id` and `theme_id`
- Returns results ordered by similarity

```sql
SELECT * FROM match_chunks(
    query_embedding := '[0.1, 0.2, ...]'::vector(384),
    match_threshold := 0.7,
    match_count := 10,
    filter_guest_id := 'guest-123'  -- optional
);
```

## Benefits Over FAISS

| Feature | FAISS (Current) | pgvector (Supabase) |
|---------|----------------|---------------------|
| **Storage** | Local files | Cloud database |
| **Deployment** | Upload files | No files needed |
| **Scalability** | Limited | Unlimited |
| **Backup** | Manual | Automatic |
| **Access** | Server only | Anywhere via API |
| **Cost** | Free (storage) | Included in Supabase |

## Migration Path

### Option 1: Use Supabase for Everything (Recommended)

**Benefits:**
- No need to upload `knowledge_base/` folder
- Simpler deployment
- Better scalability
- Automatic backups

**Steps:**
1. ‚úÖ pgvector is already enabled
2. ‚úÖ `match_chunks` function created
3. Run build script to populate embeddings:
   ```bash
   python scripts/build_knowledge_base.py
   ```
4. Update backend to use Supabase vector search (already done in `supabase_store.py`)

### Option 2: Hybrid (FAISS + Supabase)

- Use FAISS for local development
- Use Supabase for production
- Switch based on environment variable

## Testing pgvector

You can test the vector search directly in Supabase:

```sql
-- Test with a sample embedding (384 zeros as example)
SELECT * FROM match_chunks(
    query_embedding := array_fill(0.1, ARRAY[384])::vector(384),
    match_count := 5
);
```

## Current Status

- **Table exists**: ‚úÖ `chunk_embeddings` with `vector(384)` column
- **Index exists**: ‚úÖ HNSW index for fast search
- **Function exists**: ‚úÖ `match_chunks()` RPC function
- **Data**: ‚è≥ 0 chunks (need to run build script)

## Next Steps

1. **Populate embeddings**:
   ```bash
   python scripts/build_knowledge_base.py
   ```
   This will save embeddings to Supabase

2. **Update backend** to use Supabase vector search:
   - Modify `src/api/main.py` to use `SupabaseStore` instead of `VectorStore`
   - Or create a hybrid that switches based on environment

3. **Test vector search**:
   - Once embeddings are populated, test the search function
   - Verify similarity scores are reasonable

## FAQ

**Q: Do I need OpenAI for pgvector?**  
A: No! pgvector works with any embeddings. You're using sentence-transformers which is free and works perfectly.

**Q: Can I use OpenAI embeddings with pgvector?**  
A: Yes! Just change the embedding model in your code. pgvector doesn't care where the embeddings come from.

**Q: What's the difference between FAISS and pgvector?**  
A: 
- **FAISS**: Local file-based, faster for very large datasets, requires file upload
- **pgvector**: Database-based, easier deployment, better for cloud, included in Supabase

**Q: Which should I use?**  
A: For production, use **pgvector** (Supabase). It's simpler to deploy and scales better.

## Performance

- **HNSW index**: Fast approximate search (milliseconds)
- **Cosine similarity**: Standard for text embeddings
- **Filtering**: Indexed columns (guest_id, theme_id) for fast filtering

Your setup is production-ready! üöÄ

